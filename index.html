<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice/Text to Sign Language Animation System</title>
    <style>
        /* --- Design and Interface Styling --- */
        :root {
            --primary-color: #0056b3; /* Darker blue for emphasis */
            --secondary-color: #28a745; /* Green for CTAs */
            --background-color: #eef2f7; /* Light grey/blue background */
            --card-background: #ffffff;
            --highlight-color: #ffc107; /* Yellow/Orange for buttons */
            --listening-color: #dc3545; /* Red for listening state */
            --detected-color: #00bcd4; /* Cyan for successful detection */
        }
        
        body {
            font-family: 'Inter', sans-serif;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: #333;
            line-height: 1.6;
            transition: background-color 0.3s;
        }

        header {
            background-color: var(--primary-color);
            color: white;
            padding: 1.5rem 0;
            text-align: center;
            position: sticky;
            top: 0;
            z-index: 1000;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
        }

        header h1 {
            margin: 0;
            font-size: 2rem;
            text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.3);
        }

        nav a {
            color: white;
            margin: 0 20px;
            text-decoration: none;
            font-weight: 600;
            transition: color 0.3s, border-bottom 0.3s;
            padding-bottom: 5px;
        }

        nav a:hover {
            color: var(--highlight-color);
            border-bottom: 2px solid var(--highlight-color);
        }

        main {
            padding: 20px;
            max-width: 1200px;
            margin: 20px auto;
        }

        .container {
            background-color: var(--card-background);
            padding: 40px;
            margin-bottom: 30px;
            border-radius: 12px;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.1);
            scroll-margin-top: 100px; 
            border: 1px solid #ddd;
        }

        h2, h3, h4 {
            color: var(--primary-color);
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 10px;
            margin-top: 0;
        }

        #hero {
            text-align: center;
            padding: 60px 20px;
            background: linear-gradient(135deg, #e0f7fa 0%, #b3e5fc 100%);
            border-radius: 12px;
            margin-bottom: 30px;
            box-shadow: inset 0 0 10px rgba(0, 0, 0, 0.1);
        }
        
        #hero h2 {
            font-size: 2.5rem;
            color: #004d40;
            border-bottom: none;
        }

        .cta-button {
            display: inline-block;
            background-color: var(--secondary-color);
            color: white;
            padding: 12px 25px;
            text-decoration: none;
            border-radius: 8px;
            margin-top: 25px;
            font-weight: bold;
            transition: background-color 0.3s, transform 0.2s;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.15);
        }

        .cta-button:hover {
            background-color: #218838;
            transform: translateY(-2px);
        }

        .input-area, .output-area {
            margin: 20px 0;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 10px;
            background-color: #f9f9f9;
        }

        .language-select-group {
            padding: 10px 15px;
            border: 1px solid #c0d8ff;
            background-color: #e6f0ff;
            border-radius: 8px;
            margin-bottom: 15px;
            display: flex;
            gap: 15px;
            align-items: center;
        }

        #text-input {
            width: 100%;
            padding: 15px;
            margin-bottom: 15px;
            border: 2px solid #ccc;
            border-radius: 8px;
            box-sizing: border-box;
            font-size: 1rem;
            transition: border-color 0.3s;
        }

        #text-input:focus {
            border-color: var(--primary-color);
            outline: none;
        }

        #translate-btn, #voice-input-btn {
            background-color: var(--highlight-color);
            color: #333;
            border: none;
            padding: 12px 20px;
            border-radius: 8px;
            cursor: pointer;
            margin-right: 15px;
            font-weight: 600;
            transition: background-color 0.3s, transform 0.2s;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        #voice-input-btn.listening {
            background-color: var(--listening-color);
            color: white;
        }

        #translate-btn:hover:not(:disabled), #voice-input-btn:hover:not(:disabled) {
            background-color: #e0a800;
            transform: translateY(-1px);
        }
        
        #translate-btn:disabled, #voice-input-btn:disabled:not(.listening), .emotion-control-btn:disabled {
            background-color: #ccc;
            cursor: not-allowed;
            box-shadow: none;
            transform: none;
        }

        /* --- Reverse Translation Controls --- */
        .reverse-controls {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 20px;
            padding: 15px;
            border: 1px dashed #0056b3;
            border-radius: 8px;
            background-color: #f0f8ff;
            justify-content: center;
        }

        .emotion-control-btn {
            padding: 8px 15px;
            border: 2px solid;
            border-radius: 6px;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.2s;
        }

        .emotion-control-btn[data-emotion="joyful"] { border-color: #4CAF50; color: #4CAF50; background-color: #e8f5e9; }
        .emotion-control-btn[data-emotion="angry"] { border-color: #F44336; color: #F44336; background-color: #ffebee; }
        .emotion-control-btn[data-emotion="neutral"] { border-color: #9E9E9E; color: #9E9E9E; background-color: #f5f5f5; }
        .emotion-control-btn.active { box-shadow: 0 0 0 3px var(--primary-color); transform: scale(1.05); }


        /* --- Output Display --- */
        #animation-display {
            min-height: 200px;
            background-color: #e8f5e9; /* Light green background for output */
            border: 3px solid var(--secondary-color);
            border-radius: 10px;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            margin-top: 15px;
            padding: 10px;
            color: #004d40;
            font-weight: 500;
            text-align: center;
            overflow-y: auto; 
        }

        .emotion-feedback {
            text-align: center;
            margin-top: 15px;
            font-weight: bold;
            color: var(--primary-color);
            padding: 5px 10px;
            background-color: #f0f8ff;
            border-radius: 5px;
        }
        
        .simulation-step {
            color: #007bff;
            margin-top: 5px;
            font-style: italic;
            border-bottom: 1px dashed #c0d8ff;
            padding-bottom: 3px;
            width: 90%;
            max-width: 450px;
        }
        
        .simulation-step:last-of-type {
            border-bottom: none;
        }
        
        /* --- Camera/Video Setup --- */
        .camera-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
            margin-top: 20px;
        }

        #camera-video {
            width: 100%;
            max-width: 400px;
            border: 2px solid var(--detected-color);
            border-radius: 8px;
            transform: scaleX(-1); /* Mirror view for natural interaction */
            display: none; /* Hidden until camera starts */
        }
        
        #camera-status {
            font-weight: bold;
            color: var(--listening-color);
        }

        /* --- Other Sections --- */
        ol, ul {
            padding-left: 20px;
            list-style: disc;
        }

        ol li, ul li {
            margin-bottom: 10px;
        }

        .flowchart-image {
            text-align: center;
            margin: 30px 0;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 8px;
            background-color: #fdfdfd;
        }
        
        .flowchart-image img {
             max-width: 100%; 
             height: auto; 
             margin-bottom: 10px;
             border-radius: 4px;
             border: 1px solid #ccc;
        }

        footer {
            text-align: center;
            padding: 20px;
            background-color: #343a40;
            color: white;
            margin-top: 30px;
            border-top-left-radius: 10px;
            border-top-right-radius: 10px;
        }
    </style>
</head>
<body>
    <header>
        <h1>üó£Ô∏è Text to Sign Language Image Animation System</h1>
        <nav>
            <a href="#home">Home</a>
            <a href="#about">About Project</a>
            <a href="#architecture">System Working</a>
            <a href="#demo">Try Demo</a>
        </nav>
    </header>

    <main>
        <!-- Home Section -->
        <section id="home">
            <div id="hero">
                <h2>Bridging Communication Gaps with Intelligent Translation</h2>
                <p>We propose a Text to Sign Language Image Animation Generation System that facilitates inclusive communication by converting text or voice inputs into animated sign language representations. This system is enhanced with **multimodal interaction** and **emotion-aware feedback** for a natural, responsive, and empathetic experience.</p>
                <a href="#demo" class="cta-button">Start Communication</a>
            </div>
        </section>

        <!-- Demo Section -->
        <section id="demo" class="container">
            <h2>Live Demonstration (Prototype)</h2>
            <p>This prototype demonstrates **Two-Way Communication** and the core steps of the system architecture.</p>
            
            <!-- Forward Translation (Text/Voice -> Sign) -->
            <h3>1. Forward Translation (Text/Voice $\rightarrow$ Sign)</h3>
            <p>Input text or speak a message in English, Tamil, or Hindi. The system detects the emotion and generates the sign sequence.</p>

            <div class="input-area">
                
                <div class="language-select-group">
                    <label for="language-select" style="font-weight: 600;">Select Input Language:</label>
                    <select id="language-select">
                        <option value="en-US">English (US)</option>
                        <option value="ta-IN">Tamil (India)</option>
                        <option value="hi-IN">Hindi (India)</option>
                    </select>
                </div>

                <textarea id="text-input" placeholder="Type your message here (e.g., 'I am very happy today!')" rows="4"></textarea>
                
                <button id="voice-input-btn">üé§ Use Voice Input (Speech Recognition)</button>
                <button id="translate-btn">Translate to Sign Language</button>
            </div>

            <div class="output-area">
                <h3>Sign Language Animation Output</h3>
                <div id="animation-display">
                    <p style="color: #6c757d;">Input text or use voice to start the forward translation simulation.</p>
                </div>
                <p class="emotion-feedback">Emotion Detected: <span id="emotion-output">Awaiting Input</span></p>
            </div>
            
            <hr>
            
            <!-- Reverse Translation (Sign -> Text/Voice) -->
            <section id="reverse-feature">
                <h3>2. Reverse Translation (Sign $\rightarrow$ Text/Voice)</h3>
                <p>Demonstration of the system's ability to detect **Hand Gestures** and **Facial Expressions** to convert sign language back into text and voice.</p>
                
                <div class="camera-container">
                    <button id="camera-input-btn" class="cta-button" style="background-color: var(--detected-color);">üé• Start Sign Recognition (Camera Input)</button>
                    <video id="camera-video" autoplay playsinline></video>
                    <p id="camera-status">Camera Status: Off</p>
                    
                    <h4>Simulate Detected Emotion:</h4>
                    <div class="reverse-controls">
                        <button class="emotion-control-btn" data-emotion="neutral">üòê Neutral Face</button>
                        <button class="emotion-control-btn" data-emotion="joyful">üòä Happy Face</button>
                        <button class="emotion-control-btn" data-emotion="angry">üò† Angry Face</button>
                    </div>
                    
                    <div id="sign-detection-output" class="output-area" style="width: 100%; max-width: 500px; margin-top: 15px;">
                        <h4 style="border-bottom: none;">Gesture/Sign Recognition Output</h4>
                        <p id="gesture-status" style="color: var(--primary-color); font-weight: 500;">Awaiting camera start...</p>
                    </div>
                </div>
            </section>
        </section>

        <!-- About Project Section -->
        <section id="about" class="container">
            <h2>üìñ About the Project</h2>
            <p>This project aims to address the communication barriers faced by the deaf and hard-of-hearing communities by providing an intelligent, interactive translation system.</p>

            <h3>Key Features</h3>
            <ul>
                <li>**Bidirectional Communication (Two-Way):** Supports both Text/Voice $\rightarrow$ Sign Language and Sign Language $\rightarrow$ Text/Voice conversion.</li>
                <li>**Emotion-Aware Feedback:** Incorporates an **Emotion Detection System** (via voice tone or facial cues) to enhance the expressiveness and realism of the sign language output.</li>
                <li>**Output Modalities:** Capable of generating both **2D Image Sequences** and **3D Animation** (Avatar) for the final sign output.</li>
                <li>**Multilingual Support:** Planned support for primary languages, including **English, Tamil, and Hindi**.</li>
            </ul>

            <h3>üéØ Primary Objectives</h3>
            <ul>
                <li>To develop a **Voice-to-Text Conversion** module using Automatic Speech Recognition (ASR).</li>
                <li>To perform **Natural Language Processing (NLP)** to translate text into **Sign Language Gloss** (e.g., "I am happy" $\rightarrow$ "I FEEL HAPPY").</li>
                <li>To map Gloss to a **Sign Language Database** to generate gesture animations.</li>
            </ul>
        </section>

        <!-- System Architecture Section -->
        <section id="architecture" class="container">
            <h2>‚öôÔ∏è System Architecture & Working</h2>
            <p>The system comprises several interlinked modules that handle input processing, translation, emotion detection, animation generation, and user feedback.</p>
            
            <!-- IMAGE FLOW CHART ELEMENT - SOURCE IS NOW CORRECTED TO download.jpg -->
            <div class="flowchart-image">
                <img src="images/download.jpg" alt="System Architecture Flowchart" onerror="this.onerror=null; this.src='https://placehold.co/800x350/e0f7fa/0056b3?text=System+Architecture+Flowchart+Image+Not+Found';" >
                <p>Fig. 1. Flow Chart: Showing Input $\rightarrow$ NLP $\rightarrow$ Emotion Detection $\rightarrow$ Animation Generator $\rightarrow$ Output Flow</p>
            </div>

            <h3>Modular Architecture Components</h3>
            <ol>
                <li>
                    <h4>Input Module</h4>
                    <p>Handles multiple types of user input: **Text Input**, **Voice Input** (via ASR), and **Visual Input** (via camera) for facial expressions or gestures.</p>
                </li>
                <li>
                    <h4>Natural Language Processing (NLP) Module</h4>
                    <p>Processes text, performing Tokenization, Parsing, and **Sign Language Grammar Conversion**.</p>
                </li>
                <li>
                    <h4>Emotion Detection Module</h4>
                    <p>Extracts emotional context from **Text** (Sentiment analysis), **Voice** (tone, pitch), or **Facial Expressions** (using Computer Vision).</p>
                </li>
                <li>
                    <h4>Animation Generator Module</h4>
                    <p>Uses a **Sign Language Dictionary** and an **Animation Renderer** to create the final sign output, integrating **Emotion** (facial expression/posture).</p>
                </li>
                <li>
                    <h4>Output Module</h4>
                    <p>Delivers the final visual output as a **Sign Animation Display** with **Emotion-aware Enhancements**.</p>
                </li>
            </ol>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Sign Language Animation Project | Developed for Inclusive Technology | Student Project</p>
    </footer>

    <!-- JavaScript for Simulation Logic and Web Speech API -->
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const textInput = document.getElementById('text-input');
            const translateBtn = document.getElementById('translate-btn');
            const voiceInputBtn = document.getElementById('voice-input-btn');
            const languageSelect = document.getElementById('language-select');
            const cameraInputBtn = document.getElementById('camera-input-btn');
            const cameraVideo = document.getElementById('camera-video');
            const cameraStatus = document.getElementById('camera-status');
            const gestureStatus = document.getElementById('gesture-status');
            const emotionControlBtns = document.querySelectorAll('.emotion-control-btn');
            const animationDisplay = document.getElementById('animation-display');
            const emotionOutput = document.getElementById('emotion-output');

            let currentDetectedEmotion = 'neutral';
            let detectionInterval = null;
            let videoStream = null;

            // --- 1. Forward Translation (Text/Voice -> Sign) Logic ---

            const signDictionary = {
                "happy": { gloss: "FEEL HAPPY", emotion: "Joyful üòÑ", signs: "SMILE and WIPE-CHEST (joyful motion)" },
                "sad": { gloss: "FEEL SAD", emotion: "Sad üò¢", signs: "FIST-DOWNWARD-MOTION, then LOWERED-BROW (slow motion)" },
                "angry": { gloss: "FEEL ANGRY", emotion: "Angry üò†", signs: "CLAW-HAND-TO-FACE, then TENSE-BODY (fast and abrupt motion)" },
                "frustrated": { gloss: "FEEL FRUSTRATED", emotion: "Angry üò†", signs: "KNUCKLES-TO-CHIN, fast, abrupt motion" },
                "hello": { gloss: "GREET", emotion: "Neutral üë§", signs: "OPEN-HAND-WAVE (standard speed)" },
                "today": { gloss: "NOW DAY", emotion: "Neutral üë§", signs: "TWO-FINGERS-POINTING-DOWN-AND-ARCING" },
                "default": { gloss: "MESSAGE UNKNOWN", emotion: "Neutral ‚ùì", signs: "SHOULDER-SHRUG (Standard Speed)" }
            };

            const getSimulatedSign = (text) => {
                const lowerText = text.toLowerCase();
                if (lowerText.includes("happy") || lowerText.includes("joy")) return signDictionary.happy;
                if (lowerText.includes("sad") || lowerText.includes("down") || lowerText.includes("bad")) return signDictionary.sad;
                if (lowerText.includes("angry") || lowerText.includes("mad") || lowerText.includes("frustrated")) return signDictionary.angry;
                if (lowerText.includes("hello") || lowerText.includes("hi")) return signDictionary.hello;
                if (lowerText.includes("today") || lowerText.includes("now")) return signDictionary.today;
                return signDictionary.default;
            };

            const updateDisplay = (message, className = '') => {
                const p = document.createElement('p');
                p.textContent = message;
                p.className = className;
                animationDisplay.appendChild(p);
                animationDisplay.scrollTop = animationDisplay.scrollHeight; 
            };

            const translateText = async (inputText) => {
                if (!inputText || inputText.trim() === "") {
                    animationDisplay.innerHTML = '<p style="color:red;">Please enter text or use voice input.</p>';
                    emotionOutput.textContent = 'Awaiting Input';
                    return;
                }

                translateBtn.disabled = true;
                voiceInputBtn.disabled = true;
                animationDisplay.innerHTML = '<p style="font-weight: bold; color: var(--primary-color);">Starting Translation...</p>';
                emotionOutput.textContent = 'Processing...';

                const { gloss, emotion, signs } = getSimulatedSign(inputText);
                
                animationDisplay.innerHTML = '';
                
                // --- 1. Input Module Acknowledgment ---
                updateDisplay(`Input Received: "${inputText.substring(0, 40)}..."`, 'simulation-step');

                // --- 2. Emotion Detection Module Simulation (250ms) ---
                await new Promise(resolve => setTimeout(resolve, 250));
                updateDisplay(`2. Emotion Detection (from Text/Voice): ${emotion}`, 'simulation-step');
                emotionOutput.textContent = emotion;

                // --- 3. NLP Module Simulation (500ms) ---
                await new Promise(resolve => setTimeout(resolve, 500));
                updateDisplay(`3. NLP & Gloss Conversion: ‚Üí Gloss: "${gloss}"`, 'simulation-step');

                // --- 4. Animation Generator Module Simulation (750ms) ---
                await new Promise(resolve => setTimeout(resolve, 750));
                updateDisplay(`4. Animation Mapping: Mapped to signs: ${signs}`, 'simulation-step');
                
                // --- 5. Output Module Final Render (500ms) ---
                await new Promise(resolve => setTimeout(resolve, 500));
                
                const finalMessage = `<p style="font-size: 1.2rem; font-weight: bold; color: var(--secondary-color); margin-top: 15px;">‚úÖ Translation Complete (2D/3D Animation Ready)!</p>
                                     <p>Sign Language Output for: **${gloss}**</p>
                                     <p style="font-size: 0.9rem; color: #6c757d;">(Animation speed and facial cues adjusted for ${emotion.split(' ')[0]})</p>`;

                animationDisplay.innerHTML += finalMessage; 
                translateBtn.disabled = false;
                voiceInputBtn.disabled = false;
            };

            translateBtn.addEventListener('click', () => {
                translateText(textInput.value);
            });

            // Initialize Web Speech API
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            let recognition = null;

            if (SpeechRecognition) {
                recognition = new SpeechRecognition();
                recognition.continuous = false;
                recognition.interimResults = false;
                recognition.maxAlternatives = 1;

                recognition.onstart = () => {
                    voiceInputBtn.classList.add('listening');
                    voiceInputBtn.textContent = 'üî¥ Listening... Speak Now!';
                    translateBtn.disabled = true;
                };

                recognition.onend = () => {
                    voiceInputBtn.classList.remove('listening');
                    voiceInputBtn.textContent = 'üé§ Use Voice Input (Speech Recognition)';
                    translateBtn.disabled = false;
                };

                recognition.onresult = (event) => {
                    const speechResult = event.results[0][0].transcript;
                    textInput.value = speechResult;
                    animationDisplay.innerHTML = `<p style="color:var(--secondary-color); font-weight: 500;">‚úÖ Voice Input Transcribed: "${speechResult.substring(0, 30)}..."</p>`;
                };

                recognition.onerror = (event) => {
                    voiceInputBtn.classList.remove('listening');
                    voiceInputBtn.textContent = 'üé§ Use Voice Input (Speech Recognition)';
                    animationDisplay.innerHTML = `<p style="color: red;">Speech Recognition Error: ${event.error}. Please ensure microphone access is granted.</p>`;
                    translateBtn.disabled = false;
                };

                // Language selection logic
                languageSelect.addEventListener('change', () => {
                    recognition.lang = languageSelect.value;
                });
                
                // Set initial language
                recognition.lang = languageSelect.value;


                voiceInputBtn.addEventListener('click', () => {
                    try {
                        textInput.value = ''; // Clear input before listening
                        recognition.start();
                        animationDisplay.innerHTML = `<p style="color:#007bff; font-weight: bold;">üîä Waiting for speech in ${languageSelect.options[languageSelect.selectedIndex].text}...</p>`;
                    } catch (e) {
                        if (e.name !== 'InvalidStateError') {
                             animationDisplay.innerHTML = `<p style="color: red;">Microphone Error: ${e.message}</p>`;
                        }
                    }
                });

            } else {
                voiceInputBtn.disabled = true;
                voiceInputBtn.textContent = 'Voice Not Supported';
                animationDisplay.innerHTML = '<p style="color: red; margin-top: 10px;">Warning: Web Speech API not supported. Only text input works.</p>';
            }
            
            // --- 2. Reverse Translation (Sign -> Text) Logic ---

            // A. Emotion Control
            emotionControlBtns.forEach(btn => {
                btn.addEventListener('click', () => {
                    // Update state
                    currentDetectedEmotion = btn.getAttribute('data-emotion');
                    
                    // Update button appearance
                    emotionControlBtns.forEach(b => b.classList.remove('active'));
                    btn.classList.add('active');
                });
            });
            // Set initial state
            document.querySelector('.emotion-control-btn[data-emotion="neutral"]').click();

            // B. Camera and Detection Logic
            cameraInputBtn.addEventListener('click', () => {
                if (cameraVideo.style.display === 'block') {
                    // Stop camera and detection
                    if (videoStream) {
                        videoStream.getTracks().forEach(track => track.stop());
                        videoStream = null;
                    }
                    if (detectionInterval) {
                        clearInterval(detectionInterval);
                        detectionInterval = null;
                    }
                    cameraVideo.style.display = 'none';
                    cameraStatus.textContent = 'Camera Status: Off';
                    cameraStatus.style.color = var(--listening-color);
                    cameraInputBtn.textContent = 'üé• Start Sign Recognition (Camera Input)';
                    gestureStatus.textContent = 'Awaiting camera start...';
                    return;
                }

                // Start camera
                navigator.mediaDevices.getUserMedia({ video: true })
                    .then(stream => {
                        videoStream = stream;
                        cameraVideo.srcObject = stream;
                        cameraVideo.style.display = 'block';
                        cameraStatus.textContent = 'Camera Status: Active';
                        cameraStatus.style.color = var(--secondary-color);
                        cameraInputBtn.textContent = '‚èπÔ∏è Stop Camera';

                        // Start simulation detection loop
                        startDetectionSimulation();
                    })
                    .catch(err => {
                        console.error("Camera access error: ", err);
                        cameraStatus.textContent = 'Camera Error: Please allow microphone/camera access.';
                        cameraStatus.style.color = 'red';
                    });
            });

            const startDetectionSimulation = () => {
                const detectedSigns = [
                    { sign: "HELLO", text: "Hello / Namaste / Vanakkam" },
                    { sign: "GOOD", text: "Good / Acchha / Nallathu" },
                    { sign: "THANKS", text: "Thank you / Shukriya / Nandri" },
                ];
                let signIndex = 0;
                
                if (detectionInterval) clearInterval(detectionInterval);

                detectionInterval = setInterval(() => {
                    // Simulate motion tracking (most of the time)
                    const isTracking = Math.random() > 0.3; // 70% of the time, tracking motion

                    if (isTracking) {
                        gestureStatus.textContent = `Tracking Complex Motion... (Detected Emotion: ${currentDetectedEmotion.toUpperCase()})`;
                        gestureStatus.style.color = '#757575';
                    } else {
                        // Simulate successful recognition (30% of the time, or when holding sign)
                        const currentSign = detectedSigns[signIndex];

                        // Final Simulated Output Text
                        const outputText = `${currentSign.text}. [Emotion: ${currentDetectedEmotion.toUpperCase()}]`;

                        gestureStatus.innerHTML = `
                            **Sign Recognized:** ${currentSign.sign}
                            <br>
                            **Emotion Detected:** ${currentDetectedEmotion.toUpperCase()}
                            <br>
                            **Text Output:** "${outputText.substring(0, 30)}..."
                        `;
                        gestureStatus.style.color = var(--detected-color);
                        
                        // Cycle to next simulated sign
                        signIndex = (signIndex + 1) % detectedSigns.length;
                    }
                }, 1500); // Check every 1.5 seconds
            };
        });
    </script>
</body>
</html>
